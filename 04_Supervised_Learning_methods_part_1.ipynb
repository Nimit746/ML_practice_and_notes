{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7c58a2",
   "metadata": {},
   "source": [
    "<h1>Supervised Learning methods: Part 1</h1>\n",
    "\n",
    "<p>\n",
    "Supervised Learning is a task of learning to predict a numerical or categorical output for a given input sample.</br>\n",
    "\n",
    "This technique works on the special type of data that is <i><b>Labeled Data</b></i>. It simply means that the training data for the model will have some labels attached to it so that model can easily understand and map the labels to the input values with there respective outputs.</br>\n",
    "\n",
    "In this chapter there are few number of the supervised learning algorithms which are as follows:-\n",
    "\n",
    "<ol>\n",
    "<li>Linear Regression</li>\n",
    "<li>Logistc Regression</li>\n",
    "<li>Decision Trees</li>\n",
    "</ol>\n",
    "\n",
    "There are two types of data:- \n",
    "\n",
    "- Data for classification\n",
    "- Data for regression\n",
    "\n",
    "\n",
    "<p>\n",
    "Classification data is actually either Nominal or Ordinal type of data.\n",
    "\n",
    "Regression data is actually either Ratio data or the Interval data.\n",
    "\n",
    "\n",
    "In short classification works on catagorical data and regression works on numerical data.\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<h2>Linear Regression</h2>\n",
    "\n",
    "<p>\n",
    "It is a supervised learning algorithm specially to model the relationship betwee the dependent and independent variables. It focuses on constructing the linear function that outputs the value.\n",
    "</p>\n",
    "\n",
    "<b>Finding the Regression Line</b>\n",
    "\n",
    "<p>\n",
    "It works on the function which takes the value of an independent variable as an input ( Usually 'x' ) and provides the value of the dependent variable ( Usually 'y' ) as an output.</br>\n",
    "the Equation on which it works is the line equation :-</br>\n",
    "\n",
    "<b>y = mx + c</b>\n",
    "\n",
    "</br>\n",
    "where :-\n",
    "\n",
    "- m is the slope of the line\n",
    "- c is where the line meets the y axis.\n",
    "\n",
    "</br>\n",
    "Any line on a 2D plane can be defined by these two parameters.\n",
    "</br>\n",
    "\n",
    "The error is defined as the diffrence between the independent variable's actual value and the value determined by our reghression line.</br>\n",
    "We wish to find the slope 'm' and 'y' intercept 'c' such that the total cost, given by the average value of squared of the errors.\n",
    "</br>\n",
    "Usually, the data will have more than one column (components of x) that will be\n",
    "referred as x1, x2, x3… xn, which will lead to a line that has slopes of m1, m2, m3… mn across\n",
    "the n axes. Thus, the number of parameters you learn will be (n + 1) where n is the number\n",
    "of columns, or dimensions of the data. For simplicity, we will continue the explanation for a\n",
    "case where you have only one independent column, x.</br>\n",
    "The slope across each axis is given by:-\n",
    "\n",
    "![Image](./Media/linear_reg_formula.png)\n",
    "\n",
    "And the y intercept is:-\n",
    "\n",
    "![Y_intercept](./Media/Y_intercept_linear.png)\n",
    "\n",
    "Based on the y-intercept b0 and one or more sloped bk, the final equation of the line can be written as:-\n",
    "\n",
    "![Final_eq](./Media/Final_eq_linear.png)\n",
    "</p>\n",
    "\n",
    "<b>Linear Regression Using Python</b>\n",
    "\n",
    "<p>\n",
    "Here we will se and learn how to implement the linear regression model using python on a self made dataset.\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\"marks\":[34,95,64,88,99,51], \"salary\":[3400, 2900, 4250, 5000, 5100, 5600]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0352be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['marks']].values\n",
    "y = data['salary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef66040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict([[70]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict([[100],[50],[80]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031873bc",
   "metadata": {},
   "source": [
    "Here we will visualize what learnt till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81269e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (reg.coef_)\n",
    "print (reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(X, y)\n",
    "# ax.axline( (0, reg.intercept_), slope=reg.coef_ ,\n",
    "# label='regression line')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(15,7)\n",
    "plt.scatter(X, y)\n",
    "# ax.axline( (0, reg.intercept_), slope=reg.coef_ ,\n",
    "# label='regression line')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,110)\n",
    "ax.set_ylim(1000,10000)\n",
    "for point in zip(X, y):\n",
    "    ax.text(point[0][0], point[1]+5, \"(\"+str(point[0]\n",
    "[0])+\",\"+str(point[1])+\")\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8330647",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(data=X, columns=['Marks'])\n",
    "results_table['Predicted Salary'] = reg.predict(X)\n",
    "results_table['Actual Salary'] = y\n",
    "results_table['Error'] = results_table['Actual Salary']-results_table['Predicted Salary']\n",
    "results_table['Error Squared'] = results_table['Error']*results_table['Error']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0b174",
   "metadata": {},
   "source": [
    "<h2>Evaluating Linear Regression</h2>\n",
    "\n",
    "<p>\n",
    "This process is done to verify the performance of the model trained.\n",
    "\n",
    "</br>\n",
    "It can be done using the certain parameters:-\n",
    "\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "- Root Mean Squared Error\n",
    "- R^2 Score\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee412930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "mean_absolute_error = np.abs(results_table['Error']).mean()\n",
    "mean_squared_error = results_table['Error Squared'].mean()\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print (mean_absolute_error)\n",
    "print (mean_squared_error)\n",
    "print (root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
    "print(mean_squared_error(results_table['Actual Salary'], results_table['Predicted Salary']))\n",
    "print(math.sqrt(mean_squared_error(results_table['Actual Salary'], results_table['Predicted Salary'])))\n",
    "print(mean_absolute_error(results_table['Actual Salary'],\n",
    "results_table['Predicted Salary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8542603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"R Squared: %.2f\" % r2_score(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89764d2",
   "metadata": {},
   "source": [
    "<h3>Here the Linear Regression get's completed</h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h2>Logistic Regression</h2>\n",
    "\n",
    "<p>\n",
    "It is a clasification model that helps to models the <strong>probability of a data item belonging to one of the categories</strong>.</br>\n",
    "</p>\n",
    "\n",
    "<b>Line v/s Curve for Expression Probability</b>\n",
    "\n",
    "<p>\n",
    "Assuming that we have a data only in a single dimention and we have two classes for which we can try capturing this relationship using a linear regression line that gives us the probability  of a point belonging to a certain class.</br>\n",
    "The target values are either 0 or 1, where: \n",
    "\n",
    "- 0 represents the negative class\n",
    "- 1 represents the positive class\n",
    "\n",
    "As this type of data will be hard to capture using the linear rwegression line so we use the <b>Sigmoid or logistic curve</b> that tries to capture the pattern in which most of the predicted values will lie on either, \n",
    "\n",
    "- y = 0\n",
    "- y = 1\n",
    "\n",
    "and there wioll be some values within this range. this dependent value can also be treated as the probability for the point to belong to one of the classes.</br>\n",
    "\n",
    "The sigmoid or logistic function is given as:-\n",
    "\n",
    "\n",
    "![Sigmoid_function](./Media/sigmoid_1.png) </br>\n",
    "or </br></br>\n",
    "![Sigmoid_function](./Media/sigmoid_2.png)\n",
    "\n",
    "where:\n",
    "\n",
    "- -x is the input for the function in figure 1\n",
    "- where θ0, θ1, … represents the parameter (or parameters, in case of data with more\n",
    "than one column).\n",
    "\n",
    "The S-shaped curve is highly suitable for such use case. The aim of learning process is to find the θ for which we have the minimum possible error of predicting the probability. However, the cost (or error) of the model is based on the predicted class rather than the probability values.\n",
    "</p>\n",
    "\n",
    "\n",
    "<b>Learning the Parameters</b>\n",
    "\n",
    "<p>\n",
    "We use a simple iterative method to learn the parameters. Any shift in the values of the parameters causes a shift in the linear decision boundary. We begin with random initial values of the parameter, and by observing the error, we update the value of the parameter to slighlty reduce the error cost. This method is called<b>Gradient Decent</b>.\n",
    "</p>\n",
    "\n",
    "<b>Implemementation using Python</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57421312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(iris[\"data\"], columns=iris['feature_names'])\n",
    "iris_data['target'] = iris['target']\n",
    "iris_data['target'] = iris_data['target'].apply(lambda x: iris['target_names'][x])\n",
    "df = iris_data.query('target == \"setosa\" | target == \"versicolor\"')\n",
    "import seaborn as sns\n",
    "sns.FacetGrid(df, hue='target', height=5).map(plt.scatter, \"petal length (cm)\",\"petal width (cm)\").add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6aacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "x = iris_data.drop(columns=['target'])\n",
    "y = iris_data['target']\n",
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [[5.6,2.4,3.8,1.2]]\n",
    "lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33217bad",
   "metadata": {},
   "source": [
    "<b>Visualizing the Decision Boundary</b>\n",
    "<p>\n",
    "We will recreate the model using the 2 dimentions and plot a 2D chart bsed on the sepal length and sepal width\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  iris_data.query(\"target == 'setosa' | target == 'versicolor'\")[['sepal length (cm)','sepal width (cm)','target']]\n",
    "x = df.drop(columns=['target']).values\n",
    "y = df['target'].values\n",
    "y = [1 if x == 'setosa' else 0 for x in y]\n",
    "lr.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = x[:,0].min()-1, x[:,0].max()+1\n",
    "x_min, x_max = x[:, 0].min()-1, x[:,0].max()+1\n",
    "y_min, y_max = x[:, 1].min()-1, x[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "np.arange(y_min, y_max, 0.02))\n",
    "Z = lr.predict(np.c_[xx.ravel(),\n",
    "yy.ravel()]).reshape(xx.shape)\n",
    "plt.rcParams['figure.figsize']=(10,10)\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap='Blues')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1e138",
   "metadata": {},
   "source": [
    "Here the Logistic Regression is completed.\n",
    "\n",
    "---\n",
    "\n",
    "<h2><b>Decision Tree</b></h2>\n",
    "\n",
    "<p>\n",
    "It is a machine learning model that makes the prediction by the set of decisions, where the rules can be written down in a flowchart manner.\n",
    "</br>\n",
    "The visual diagram of this tree is similar to the tree data structure where the nodes contain the rules and conditions for the decision tree.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3><b>Building a Decsion Tree</b></h3>\n",
    "\n",
    "<p>\n",
    "The learning process of a decision tree is a recurssive process, where each recurtion takes the tree to a new condition on which the data has to be trained, as the data will find the optimal path to get to the prediction by passing through several conditions. It is basically used for classifying.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3><b>Picking the Splitting Attribute</b></h3>\n",
    "\n",
    "- Decision tree learning is a **recursive process**\n",
    "\n",
    "- At each recursion, the algorithm tries to find the **best possible split**\n",
    "\n",
    "- A split is made if there's enough data with enough variation of target classes\n",
    "\n",
    "<b>Leaf Nodes</b>\n",
    "\n",
    "- A **leaf node** is created when:\n",
    "   - Training data is too small, OR\n",
    "   - Data belongs to the same target class\n",
    "   - The leaf node is assigned the **majority class label**\n",
    "\n",
    "<b>Splitting Criterion</b>\n",
    "\n",
    "- **Picking the splitting attribute is the core of the algorithm**\n",
    "\n",
    "<b>Entropy</b>\n",
    "\n",
    "- **Entropy** measures the amount of randomness or uncertainty in data\n",
    "   - Formula: `H(X) = -Σ(p(x) × log₂(p(x)))`\n",
    "   - Entropy = 0 when sample is completely homogeneous (same class)\n",
    "   - Entropy = 1 when sample is equally divided among all classes\n",
    "\n",
    "- Goal is to **minimize entropy** (lower randomness) and create **purer nodes**\n",
    "\n",
    "<b>Information Gain</b>\n",
    "\n",
    "- **Information gain** measures how much a feature tells us about the class\n",
    "   - Select the attribute with **highest information gain**\n",
    "\n",
    "<b>Gini Index</b>\n",
    "\n",
    "- **Gini index** is used for continuous data attributes\n",
    "   - Measures impurity\n",
    "   - Higher Gini = higher homogeneity\n",
    "   - Used by **CART** (Classification and Regression Tree) to create binary splits\n",
    "\n",
    "<b>Implementation</b>\n",
    "\n",
    "- In Scikit-learn, use the **criterion hyperparameter** to select splitting criteria\n",
    "\n",
    "\n",
    "<h3><b>Decision Tree Implementation</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris_data = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "iris_data['target'] = iris['target']\n",
    "iris_data['target'] = iris_data['target'].apply(lambda x: iris['target_names'][x])\n",
    "print(iris_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e294de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris_data[['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']]\n",
    "y = iris_data['target']\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61610575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=10)\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045ad9c",
   "metadata": {},
   "source": [
    "<h2>Decision Tree and the chapter has been completed successfully!</h2>\n",
    "\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
