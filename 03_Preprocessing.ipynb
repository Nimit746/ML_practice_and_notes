{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6ef5ef",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>\n",
    "\n",
    "<p>\n",
    "Preprocessing is a phase or a process applied after the Exploratory Data Analysis and before machine learning training. Here the data is started to modify already so that clean and clear data can be used to train the models.\n",
    "</p>\n",
    "\n",
    "\n",
    "<h2>Normalization (Also known as Feature Scaling)</h2>\n",
    "\n",
    "<p>\n",
    "It is a part of the preprocessing which is mandtory to applyso that the standard data can be given to models for training.\n",
    "</br>\n",
    "It has two types of scaling methods which are widely used:-\n",
    "<ol>\n",
    "<li>Min-Max Scaling</li>\n",
    "<li>Standard Scaling</li>\n",
    "</ol>\n",
    "\n",
    "It is used tonormalize the data so that the features are in a similar range. It is important for those algortihms which are highly affected by the distribution of the shape or are based on the vector- or distance-based computations.</br>\n",
    "\n",
    "<p>\n",
    "For using the scaling methods we wil use the same dataframe as created in the EDA file before that is <strong>02_Exploratory_data_analysis.ipynb</strong>\n",
    "</p>\n",
    "</p>\n",
    "\n",
    "<h2>Min-Max Scaling</h2>\n",
    "\n",
    "<p>\n",
    "It transforms each feature by compressing it down to a scale wheere the minimum number in the dataset maps to 0 and the maximum number in the dataset maps to 1.</br>\n",
    "The transformation is given by:-</br>\n",
    "\n",
    "![Image](Media/image.png)\n",
    "\n",
    "Feature range (min, max) can be configured if required.\n",
    "</p>\n",
    "\n",
    "\n",
    " ### fit() calculates parameters from the data, but does not change the data itself.\n",
    " ### transform() uses the parameters learned during fit() to actually modify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Age': {0: 28, 1: 23, 2: 19},\n",
    "'Gender_F': {0: 0.0, 1: 0.0, 2: 1.0},\n",
    "'Gender_M': {0: 1.0, 1: 1.0, 2: 0.0},\n",
    "'Degree_encoded': {0: 0.0, 1: 2.0, 2: 1.0}})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[['Age']])\n",
    "df['Age'] = scaler.transform(df[['Age']])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d9442",
   "metadata": {},
   "source": [
    "<h2>Standard Scaling</h2>\n",
    "\n",
    "<p>\n",
    "It transforms each feature values by removing the mean and scaling to unit variance. the value thus represent the z-value with respect to the mean and variance of the column.</br>\n",
    "The transformation is given by:-</br>\n",
    "\n",
    "![Image](Media/image2.png)\n",
    "\n",
    "where\n",
    "- Î¼ is the mean\n",
    "- s is the standard deviation of the samples\n",
    "\n",
    "We can take the original values of the Age columns and scale it using StandardScaler\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "arr = scaler.fit_transform(df[['Age']])\n",
    "# df['Age'] = scaler.transform(df[['Age']])\n",
    "print(df)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b50821",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4f571",
   "metadata": {},
   "source": [
    "<h2>Preprocessing Text</h2>\n",
    "\n",
    "<p>\n",
    "For preprocessing the text we need to use NLP ( Natural Language Processing ). NLP is a subject of processing and generating text. the most popular library used in python for text is NLTK ( Natural Language Toolkit ).\n",
    "</p>\n",
    "\n",
    "<h2>Five-Step of NLTK Pipeline</h2>\n",
    "\n",
    "<p>\n",
    "There are majorly five steps in using the NLTK pipeline to preprocess the textual data.\n",
    "<ol>\n",
    "<li>Segmentation</li>\n",
    "<li>Tokenization</li>\n",
    "<li>Stemming and Lemmatization</li>\n",
    "<li>Removing Stopwords</li>\n",
    "<li>Preparing Word Vectors</li>\n",
    "</ol>\n",
    "</p>\n",
    "\n",
    "<h3>Segmentation</h3>\n",
    "\n",
    "<p>\n",
    "It is the process of finding the boundaries of the sentence.</br>\n",
    "Mostly a complex regular expession is used to find out the boundaries of the sentence.\n",
    "</p>\n",
    "\n",
    "<h3>Tokenization</h3>\n",
    "\n",
    "<p>\n",
    "It breaks the sentence or a sequence inot the indivisual components or units called Tokens.\n",
    "</br>\n",
    "NLTK basic code below for <strong>Tokenization</strong>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e545aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Let's learn machine learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t.lower() for t in word_tokenize(\"Let's learn the greatest machine learning!\")]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c230f",
   "metadata": {},
   "source": [
    "<h3><i>Stemming and Lemmatization</i></h3>\n",
    "\n",
    "<p>\n",
    "For grammatical reasons, the same word root can be present in different forms in the text. in most cases, they lead to a similar meaning, for example, work, wroking, worked. </br>\n",
    "One popular method used for stemming is <strong>Porter's Stemmer</strong>. It performs the following set of rule based operations:-</br>\n",
    "\n",
    "```\n",
    "SSES -> SS\n",
    "IES -> I\n",
    "SS -> SS\n",
    "S ->  \n",
    "```\n",
    "\n",
    "</p>\n",
    "\n",
    "It's implimentation is as follows:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for token in tokens:\n",
    "    print(f'{token} : {stemmer.stem(token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ed389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For larger programs this will work\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111fde9",
   "metadata": {},
   "source": [
    "<h3>Removing Stopwords</h3>\n",
    "\n",
    "<p>\n",
    "There are some words which produce noise and slow down the process of training so they must be removed.\n",
    "</br>These words are called as <b>Stopwords</b>.\n",
    "</br>NLTK has the complete list of stopwords which can be used to filter out these words from the text and make the procewss faster.</br>\n",
    "Below is the sample code for the topic:-\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stp = stopwords.words('english')\n",
    "for token in stemmed_tokens:\n",
    "    if token in eng_stp:\n",
    "        stemmed_tokens.remove(token)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7bf3f",
   "metadata": {},
   "source": [
    "<h3>Word Vectors</h3>\n",
    "\n",
    "<p>\n",
    "Just like the OnehotEncoding and LabelEncoding this also converts the text to the numbers, however , the differenc is that OneHotEncoding and LabelEncoding converts the catagorical data to the numbers but here the text is converted to the vector on the basis of the occurance of that word in the sentence.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "For this the function or the class used in opython is <strong>CountVectorizer</strong> present in the <strong>sklearn.feature_extraction.text</strong>\n",
    "</p>\n",
    "Sample code is written below:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68482a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "eng_stp  = stopwords.words('english')\n",
    "data = [\"Let's learn Machine Learning Now\",\"The Machines are Learning\",\"It is Learning Time\"]\n",
    "tokens = [word_tokenize(d) for d in data] # This creates a 2D array for the tokenized words in each line present in the data list.\n",
    "# print(tokens)\n",
    "tokens = [[word.lower() for word in line] for line in tokens]\n",
    "\n",
    "for i, line in enumerate(tokens):\n",
    "    for word in line:\n",
    "        if word in stopwords.words('english'):\n",
    "            line.remove(word)\n",
    "    tokens[i] = ' '.join(line)\n",
    "    \n",
    "matrix = CountVectorizer()\n",
    "X = matrix.fit_transform(tokens).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "arr = pd.DataFrame(X, columns=matrix.get_feature_names_out())\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc16fa3",
   "metadata": {},
   "source": [
    "<h2>Preprocessing Images</h2>\n",
    "\n",
    "<p>\n",
    "Here we will se the preprocessing of the Images.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b66552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = plt.imread(\"D:\\\\Coding\\\\Python\\\\ML_practice\\\\Media\\\\dog.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f799820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06df2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = img[200:470, 100:500, :]\n",
    "print(img.shape)\n",
    "plt.imshow(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io,filters\n",
    "edges = filters.sobel(img)  # It is used to create the image emphasising edges of the original image, which can act as a part of the pipeline of a system involving recognition or classification\n",
    "io.imshow(edges)\n",
    "io.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
